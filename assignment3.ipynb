{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181bbe29-26f4-4c4a-a9e9-0053263bff52",
   "metadata": {},
   "source": [
    "# Assignment: Scalable Processing\n",
    "## Yelp Reviews and Authenticity\n",
    "\n",
    "course name | by ___ | ____@itu.dk | date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92690678-bb97-43d7-ab27-4a6f8db87079",
   "metadata": {},
   "source": [
    "## Connecting to the Spark Cluster job using the two JobParameters.json\n",
    "\n",
    "To connect this jupyter notebook with your Spark cluster, we need to tell jupyter how it can access the spark cluster. Below code accomplishes that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483a774-a0ca-4873-a2ee-51d2fcd30ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# DO NOT CHANGE ANYTHING HERE.\n",
    "# IF YOU HAVE PROBLEMS, CHECK THE ASSIGNMENT GUIDE CAREFULLY \n",
    "#####################################################################\n",
    "    \n",
    "FIRST_EXECUTION = False\n",
    "# Only execute this cell once.\n",
    "if not FIRST_EXECUTION:\n",
    "    FIRST_EXECUTION = True\n",
    "    import os, json, pyspark\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession, functions as F\n",
    "\n",
    "    # Two files are automatically read: JobParameters.json for the Spark Cluster job using a temporary spark instance\n",
    "    # and JobParameters.json for the Jupyter Lab job to extract the hostname of the cluster. \n",
    "\n",
    "    MASTER_HOST_NAME = None\n",
    "\n",
    "    # Open the parameters Jupyter Lab app was launched with\n",
    "    with open('/work/JobParameters.json', 'r') as file:\n",
    "        JUPYTER_LAB_JOB_PARAMS = json.load(file)\n",
    "        # from pprint import pprint; pprint(JUPYTER_LAB_JOB_PARAMS) \n",
    "        for resource in JUPYTER_LAB_JOB_PARAMS['request']['resources']:\n",
    "            if 'hostname' in resource.keys():\n",
    "                MASTER_HOST_NAME = resource['hostname']\n",
    "\n",
    "    MASTER_HOST = f\"spark://{MASTER_HOST_NAME}:7077\"\n",
    "\n",
    "    conf = SparkConf().setAll([\n",
    "            (\"spark.app.name\", 'reading_job_params_app'), \n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "        ])\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "\n",
    "    CLUSTER_PARAMETERS_JSON_DF = spark.read.option(\"multiline\",\"true\").json('/work/JobParameters.json')\n",
    "\n",
    "    # Extract cluster info from the specific JobParameters.json\n",
    "    NODES = CLUSTER_PARAMETERS_JSON_DF.select(\"request.replicas\").first()[0]\n",
    "    CPUS_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.cpu\").first()[0] - 1\n",
    "    MEM_PER_NODE = CLUSTER_PARAMETERS_JSON_DF.select(\"machineType.memoryInGigs\").first()[0]\n",
    "\n",
    "    CLUSTER_CORES_MAX = CPUS_PER_NODE * NODES\n",
    "    CLUSTER_MEMORY_MAX = MEM_PER_NODE * NODES \n",
    "\n",
    "    EXECUTOR_CORES = CPUS_PER_NODE - 1  # set cores per executor on worker node\n",
    "    EXECUTOR_MEMORY = int(\n",
    "        MEM_PER_NODE / (CPUS_PER_NODE / EXECUTOR_CORES) * 0.5\n",
    "    )  # set executor memory in GB on each worker node\n",
    "\n",
    "    # Make sure there is a dir for spark logs\n",
    "    if not os.path.exists('spark_logs'):\n",
    "        os.mkdir('spark_logs')\n",
    "\n",
    "    conf = SparkConf().setAll(\n",
    "        [\n",
    "            (\"spark.app.name\", 'spark_assignment'), # Change to your liking \n",
    "            (\"spark.sql.caseSensitive\", False), # Optional: Make queries strings sensitive to captialization\n",
    "            (\"spark.master\", MASTER_HOST),\n",
    "            (\"spark.cores.max\", CLUSTER_CORES_MAX),\n",
    "            (\"spark.executor.cores\", EXECUTOR_CORES),\n",
    "            (\"spark.executor.memory\", str(EXECUTOR_MEMORY) + \"g\"),\n",
    "            (\"spark.eventLog.enabled\", True),\n",
    "            (\"spark.eventLog.dir\", \"spark_logs\"),\n",
    "            (\"spark.history.fs.logDirectory\", \"spark_logs\"),\n",
    "            (\"spark.deploy.mode\", \"cluster\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    ## check executor memory, taking into accout 10% of memory overhead (minimum 384 MiB)\n",
    "    CHECK = (CLUSTER_CORES_MAX / EXECUTOR_CORES) * (\n",
    "        EXECUTOR_MEMORY + max(EXECUTOR_MEMORY * 0.10, 0.403)\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        int(CHECK) <= CLUSTER_MEMORY_MAX\n",
    "    ), \"Executor memory larger than cluster total memory!\"\n",
    "\n",
    "    # Stop previous session that was just for loading cluster params\n",
    "    spark.stop()\n",
    "\n",
    "    # Start new session with above config, that has better resource handling\n",
    "    spark = SparkSession.builder.config(conf=conf)\\\n",
    "                                .getOrCreate()\n",
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbb4de-8b40-4364-b8d3-2cc488b16cbf",
   "metadata": {},
   "source": [
    "Click on the \"SparkMonitor\" tab at the top in Jupyter Lab to see the status of running code on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de3dec-d95c-44b9-a996-12e97cc34c2b",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Here we specify where the yelp datasets are located on UCloud and read then using the spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa96702-5902-4482-9cd2-77d6f5da0f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the business and review files\n",
    "# This is the path to the shared datasets provided by adding an the dataset input folder\n",
    "# when submitting the spark cluster job.\n",
    "business = spark.read.json('file:////work/yelp/yelp_academic_dataset_business.json') # Use the file:/// prefix to indicate we want to read from the cluster's filesystem\n",
    "business = business.persist()\n",
    "# Persist 2 commonly used dataframes since they're used for later computations\n",
    "# https://sparkbyexamples.com/spark/spark-difference-between-cache-and-persist/\n",
    "\n",
    "users = spark.read.json(\"file:////work/yelp/yelp_academic_dataset_user.json\")\n",
    "\n",
    "reviews = spark.read.json('file:////work/yelp/yelp_academic_dataset_review.json')\n",
    "reviews = reviews.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f6184-410e-4a04-a4a1-7b080353dc1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of rows with no sampling:\n",
    "reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31d1a2-3571-49cd-af5a-98d6d23027de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL:\n",
    "# Reduce resource usage and make queries run faster\n",
    "# by only using a small sample of the dataframe\n",
    "# and overwriting previous variable \"df\".\n",
    "# Useful while developing, not so much to\n",
    "# provide final answers. Therefore: Remember to \n",
    "# to re-read the df when done developing code using\n",
    "# df = spark.read etc like above.\n",
    "reviews = reviews.sample(withReplacement=False, fraction=1/50)\n",
    "\n",
    "# Get number of rows after sampling:\n",
    "reviews.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20087cdb-d6f2-4aab-9fe2-bf3f072044db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "business.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d258fa6-1402-4c3a-a252-a9dbfd4a87d9",
   "metadata": {},
   "source": [
    "Example: Say we're only interested in reviews of good mexican restaurants in Arizona. You can delete this when you do your own thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07983acb-15d8-45ea-9d12-3bc8759280b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter to only Arizona businesses with \"Mexican\" as part of their categories\n",
    "az_mex = business.filter(business.state == \"AZ\")\\\n",
    "                .filter(business.categories.rlike(\"Mexican\"))\\\n",
    "                .select(\"business_id\", \"name\")\n",
    "\n",
    "# Join with the reviews\n",
    "az_mex_rs = reviews.join(az_mex, on=\"business_id\", how=\"inner\")\n",
    "\n",
    "# Filter to only 5 star reviews\n",
    "good_az_mex_rs = az_mex_rs.filter(az_mex_rs.stars == 5)\\\n",
    "                        .select(\"name\",\"text\")\n",
    "\n",
    "# Print the top 20 rows of the DataFrame\n",
    "good_az_mex_rs.show()\n",
    "\n",
    "# Convert to pandas (local object) and save to local file system\n",
    "good_az_mex_rs.toPandas().to_csv(\"good_az_reviews.csv\", header=True, index=False, encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}